\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{cambridge2020linguaskill}
\citation{CouncilOfEurope2001CEFR}
\citation{bert_1,raina20_interspeech}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:introduction}{{1}{2}{Introduction}{chapter.1}{}}
\citation{bannò2022proficiencyassessmentl2spoken,bannò2022l2proficiencyassessmentusing,mcknight_civelekoglu_gales_banno_liusie_knill_2023}
\citation{ma2025assessment}
\citation{eltanbouly2025tratestraitspecificrubricassistedcrossprompt}
\citation{knill2024sandi}
\citation{xu2025qwen25omnitechnicalreport}
\citation{bernstein90_icslp}
\citation{qian12c_interspeech}
\citation{bert_1}
\citation{devlin2019bertpretrainingdeepbidirectional}
\citation{yang2020xlnetgeneralizedautoregressivepretraining}
\citation{raina20_interspeech}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:lit_review}{{2}{4}{Literature Review}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Spoken Language Assessment}{4}{section.2.1}\protected@file@percent }
\newlabel{sec:spoken_language_assessment}{{2.1}{4}{Spoken Language Assessment}{section.2.1}{}}
\citation{baevski2020wav2vec20frameworkselfsupervised}
\citation{hsu2021hubertselfsupervisedspeechrepresentation}
\citation{bannò2022proficiencyassessmentl2spoken}
\citation{bannò2022l2proficiencyassessmentusing}
\citation{mcknight_civelekoglu_gales_banno_liusie_knill_2023}
\citation{ma2025assessment}
\citation{Ludlow2020OfficialQuick}
\citation{knill2024sandi}
\citation{liusie2023mitigatingwordbiaszeroshot}
\citation{balestriero2023cookbookselfsupervisedlearning}
\citation{devlin2019bertpretrainingdeepbidirectional}
\citation{chen2020simpleframeworkcontrastivelearning}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Self Supervised Learning}{5}{section.2.2}\protected@file@percent }
\newlabel{sec:self_supervised_learning}{{2.2}{5}{Self Supervised Learning}{section.2.2}{}}
\citation{devlin2019bertpretrainingdeepbidirectional}
\citation{baevski2020wav2vec20frameworkselfsupervised}
\citation{hsu2021hubertselfsupervisedspeechrepresentation}
\citation{radford2022robustspeechrecognitionlargescale}
\citation{chu2023qwenaudioadvancinguniversalaudio}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Speech Large Language Models}{6}{section.2.3}\protected@file@percent }
\newlabel{sec:speech_large_language_models}{{2.3}{6}{Speech Large Language Models}{section.2.3}{}}
\citation{hu2021loralowrankadaptationlarge}
\citation{rubenstein2023audiopalmlargelanguagemodel}
\citation{tang2024salmonngenerichearingabilities}
\citation{chu2023qwenaudioadvancinguniversalaudio}
\citation{chu2024qwen2audiotechnicalreport}
\citation{ghosh2024gamalargeaudiolanguagemodel}
\citation{microsoft2025phi4minitechnicalreportcompact}
\citation{xu2025qwen25omnitechnicalreport}
\citation{sakshi2024mmaumassivemultitaskaudio}
\citation{peng21e_interspeech,inproceedings_wu_mdd,xu21k_interspeech}
\citation{kim2022automaticpronunciationassessmentusing}
\citation{fu2024pronunciationassessmentmultimodallarge}
\citation{kim2022automaticpronunciationassessmentusing,fu2024pronunciationassessmentmultimodallarge}
\citation{ma2025assessment}
\citation{page1966}
\citation{aes_gpt3}
\citation{gpt3}
\citation{yancey-etal-2023-rating}
\citation{openai2024gpt4technicalreport}
\citation{naismith-etal-2023-automated}
\citation{mansour2024largelanguagemodelsautomatically}
\citation{touvron2023llama2openfoundation}
\citation{stahl2024exploringllmpromptingstrategies}
\citation{mathias-bhattacharyya-2018-asap}
\citation{do2024autoregressivescoregenerationmultitrait}
\citation{raffel2023exploringlimitstransferlearning}
\citation{banno-etal-2024-gpt}
\citation{beltagy2020longformerlongdocumenttransformer}
\citation{lee2024unleashinglargelanguagemodels}
\citation{chu2025rationaleessayscoresenhancing}
\citation{eltanbouly2025tratestraitspecificrubricassistedcrossprompt}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Mispronunciation Detection}{8}{section.2.4}\protected@file@percent }
\newlabel{sec:mispronunciation_detection}{{2.4}{8}{Mispronunciation Detection}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Automatic Essay Scoring (AES)}{8}{section.2.5}\protected@file@percent }
\newlabel{sec:automatic_essay_scoring}{{2.5}{8}{Automatic Essay Scoring (AES)}{section.2.5}{}}
\citation{eltanbouly2025tratestraitspecificrubricassistedcrossprompt}
\citation{mcinerney2023chillzeroshotcustominterpretable}
\citation{benara2024craftinginterpretableembeddingsasking}
\citation{balek2025llmbasedfeaturegenerationtext}
\citation{sam2025predictingperformanceblackboxllms}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Question-based LLM-derived Interpretable Features}{9}{section.2.6}\protected@file@percent }
\newlabel{sec:llm_derived_interpretable_features}{{2.6}{9}{Question-based LLM-derived Interpretable Features}{section.2.6}{}}
\citation{eltanbouly2025tratestraitspecificrubricassistedcrossprompt,mcinerney2023chillzeroshotcustominterpretable,benara2024craftinginterpretableembeddingsasking,balek2025llmbasedfeaturegenerationtext}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:methodology}{{3}{11}{Methodology}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem Setup}{11}{section.3.1}\protected@file@percent }
\newlabel{sec:problem_setup}{{3.1}{11}{Problem Setup}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Question-based Approach}{12}{section.3.2}\protected@file@percent }
\newlabel{sec:question_based_approach}{{3.2}{12}{Question-based Approach}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Question Set Design}{12}{subsection.3.2.1}\protected@file@percent }
\newlabel{subsec:question_set_design}{{3.2.1}{12}{Question Set Design}{subsection.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Example of a multiple-choice question with three options.}}{12}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example_question}{{3.1}{12}{Example of a multiple-choice question with three options}{figure.caption.2}{}}
\citation{benara2024craftinginterpretableembeddingsasking}
\citation{xu2025qwen25omnitechnicalreport}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Feature Extraction via Speech LLMs}{13}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsec:feature_extraction}{{3.2.2}{13}{Feature Extraction via Speech LLMs}{subsection.3.2.2}{}}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Final Regression Model}{14}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsec:final_regression_model}{{3.2.3}{14}{Final Regression Model}{subsection.3.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Schematic illustration of the question-based feature extraction process. The speech LLM is prompted to answer each question for every audio sample. The output probabilities for each option are collected into a table to form the interpretable feature matrix $\mathbf  {Z}$, which is then used to train a regression model to predict the target variable (part scores).}}{15}{figure.caption.3}\protected@file@percent }
\newlabel{fig:question_based_approach}{{3.2}{15}{Schematic illustration of the question-based feature extraction process. The speech LLM is prompted to answer each question for every audio sample. The output probabilities for each option are collected into a table to form the interpretable feature matrix $\mathbf {Z}$, which is then used to train a regression model to predict the target variable (part scores)}{figure.caption.3}{}}
\citation{knill2024sandi}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{16}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:experiments}{{4}{16}{Experiments}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dataset}{16}{section.4.1}\protected@file@percent }
\newlabel{sec:dataset}{{4.1}{16}{Dataset}{section.4.1}{}}
\citation{qian2024sandi}
\citation{radford2022robustspeechrecognitionlargescale}
\citation{devlin2019bertpretrainingdeepbidirectional}
\citation{qian2024sandi}
\citation{xu2025qwen25omnitechnicalreport}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Baselines}{17}{section.4.2}\protected@file@percent }
\newlabel{sec:baselines}{{4.2}{17}{Baselines}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}BERT Baseline}{17}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Speech LLM Representations}{17}{subsection.4.2.2}\protected@file@percent }
\citation{ma2025assessment}
\citation{ma2025assessment}
\citation{cambridge2020linguaskill}
\citation{eltanbouly2025tratestraitspecificrubricassistedcrossprompt}
\citation{cambridge2020linguaskill}
\citation{cambridge2020linguaskill}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Question Sets}{18}{section.4.3}\protected@file@percent }
\newlabel{sec:question_sets}{{4.3}{18}{Question Sets}{section.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of question sets used in experiments.}}{18}{table.caption.4}\protected@file@percent }
\newlabel{table:question_sets}{{4.1}{18}{Overview of question sets used in experiments}{table.caption.4}{}}
\citation{kwon2023efficientmemorymanagementlarge}
\citation{knill2024sandi}
\citation{wolf2020huggingfacestransformersstateoftheartnatural}
\citation{apache_parquet}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Implementation Details}{19}{section.4.4}\protected@file@percent }
\newlabel{sec:implementation_details}{{4.4}{19}{Implementation Details}{section.4.4}{}}
\citation{cheng2023batchpromptingefficientinference}
\citation{qian2024sandi,ma2025assessment}
\citation{qian2024sandi,ma2025assessment}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Prompt template used for sequential question inference with the speech LLM.}}{20}{figure.caption.5}\protected@file@percent }
\newlabel{fig:sequential_inference_prompt}{{4.1}{20}{Prompt template used for sequential question inference with the speech LLM}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Calibration and Evaluation Metrics}{20}{section.4.5}\protected@file@percent }
\newlabel{chap:calibration_and_evaluation_metrics}{{4.5}{20}{Calibration and Evaluation Metrics}{section.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of the two-stage calibration procedure used to align predicted scores with ground-truth scores. First, predictions from all files of a given speaker are averaged to obtain speaker-level scores (this is relevant for parts 1 and 5 where multiple files exist per speaker). Next, a linear calibration is applied to each part's predicted scores. The overall predicted score is computed as the average of the calibrated part scores, followed by a second linear calibration to align with ground-truth overall scores. Note that each linear calibration has its own slope and intercept parameters that are fitted on the dev set.}}{21}{figure.caption.6}\protected@file@percent }
\newlabel{fig:calibration_procedure}{{4.2}{21}{Illustration of the two-stage calibration procedure used to align predicted scores with ground-truth scores. First, predictions from all files of a given speaker are averaged to obtain speaker-level scores (this is relevant for parts 1 and 5 where multiple files exist per speaker). Next, a linear calibration is applied to each part's predicted scores. The overall predicted score is computed as the average of the calibrated part scores, followed by a second linear calibration to align with ground-truth overall scores. Note that each linear calibration has its own slope and intercept parameters that are fitted on the dev set}{figure.caption.6}{}}
\citation{qian2024sandi}
\citation{ma2025assessment}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results and Discussion}{22}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:results_discussion}{{5}{22}{Results and Discussion}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Comparison of Transformations}{22}{section.5.1}\protected@file@percent }
\newlabel{sec:comparison_of_transformations}{{5.1}{22}{Comparison of Transformations}{section.5.1}{}}
\citation{liusie2023mitigatingwordbiaszeroshot}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of linear models using different feature transformations applied to the \emph  {Initial Question Set}.}}{23}{table.caption.7}\protected@file@percent }
\newlabel{table:feature_transformations}{{5.1}{23}{Performance comparison of linear models using different feature transformations applied to the \emph {Initial Question Set}}{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Comparison of Models}{23}{section.5.2}\protected@file@percent }
\newlabel{sec:comparison_of_models}{{5.2}{23}{Comparison of Models}{section.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Performance comparison of various regression models using features from the \emph  {Initial Question Set}.}}{23}{table.caption.8}\protected@file@percent }
\newlabel{table:regression_models}{{5.2}{23}{Performance comparison of various regression models using features from the \emph {Initial Question Set}}{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Comparison of Feature Sets}{24}{section.5.3}\protected@file@percent }
\newlabel{sec:comparison_of_feature_sets}{{5.3}{24}{Comparison of Feature Sets}{section.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for overall scores).}}{24}{table.caption.9}\protected@file@percent }
\newlabel{table:feature_sets}{{5.3}{24}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for overall scores)}{table.caption.9}{}}
\citation{galton_1886_1449548}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Visual Inspection}{26}{section.5.4}\protected@file@percent }
\newlabel{sec:visual_inspection}{{5.4}{26}{Visual Inspection}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Predictions versus ground truth scores for the \emph  {Initial Question Set}. The diagonal line represents perfect predictions, while the fitted line shows the actual model behaviour.}}{26}{figure.caption.10}\protected@file@percent }
\newlabel{fig:predictions_vs_ground_truth_original}{{5.1}{26}{Predictions versus ground truth scores for the \emph {Initial Question Set}. The diagonal line represents perfect predictions, while the fitted line shows the actual model behaviour}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Interpretable Feature Analysis}{26}{section.5.5}\protected@file@percent }
\newlabel{sec:interpretable_feature_analysis}{{5.5}{26}{Interpretable Feature Analysis}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Regression lines of predictions versus ground truth scores across all feature sets. The plots show how different feature representations affect the regression line, with higher-dimensional features with higher PCC generally producing larger slopes that are able to better capture extreme target values.}}{27}{figure.caption.11}\protected@file@percent }
\newlabel{fig:predictions_vs_ground_truth_all}{{5.2}{27}{Regression lines of predictions versus ground truth scores across all feature sets. The plots show how different feature representations affect the regression line, with higher-dimensional features with higher PCC generally producing larger slopes that are able to better capture extreme target values}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Correlation Analysis}{27}{subsection.5.5.1}\protected@file@percent }
\newlabel{subsec:correlation_analysis}{{5.5.1}{27}{Correlation Analysis}{subsection.5.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Correlation matrix between features and the target variable from the \emph  {Initial Question Set}. Each question has three features corresponding to the probabilities of the ``High'', ``Medium'', and ``Low'' options.}}{28}{figure.caption.12}\protected@file@percent }
\newlabel{fig:correlation_matrix_original}{{5.3}{28}{Correlation matrix between features and the target variable from the \emph {Initial Question Set}. Each question has three features corresponding to the probabilities of the ``High'', ``Medium'', and ``Low'' options}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Correlation matrix between features and the target variable from the \emph  {Rubric-based Question Set}. Each rubric dimension has six features corresponding to the probabilities of grades A through F.}}{29}{figure.caption.13}\protected@file@percent }
\newlabel{fig:correlation_matrix_rubric}{{5.4}{29}{Correlation matrix between features and the target variable from the \emph {Rubric-based Question Set}. Each rubric dimension has six features corresponding to the probabilities of grades A through F}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Regression Coefficients}{29}{subsection.5.5.2}\protected@file@percent }
\newlabel{subsec:regression_coefficients}{{5.5.2}{29}{Regression Coefficients}{subsection.5.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Regression coefficients for the \emph  {Rubric-based Question Set}. Each rubric dimension (pronunciation and fluency: pf, language resource: lr, discourse management: dm) has six grade options (A-F). The counter-intuitive coefficient signs demonstrate that high-grade features (A) don't consistently receive positive coefficients, and low-grade features (F) don't consistently receive negative coefficients, illustrating the impact of multicollinearity among features.}}{30}{figure.caption.14}\protected@file@percent }
\newlabel{fig:regression_coefficients}{{5.5}{30}{Regression coefficients for the \emph {Rubric-based Question Set}. Each rubric dimension (pronunciation and fluency: pf, language resource: lr, discourse management: dm) has six grade options (A-F). The counter-intuitive coefficient signs demonstrate that high-grade features (A) don't consistently receive positive coefficients, and low-grade features (F) don't consistently receive negative coefficients, illustrating the impact of multicollinearity among features}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Question-level Scores}{30}{subsection.5.5.3}\protected@file@percent }
\newlabel{subsec:question_scores}{{5.5.3}{30}{Question-level Scores}{subsection.5.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Illustration of the question-level scores methodology for obtaining analytic scores for each question. Separate linear models are trained to predict the same target variable (part score) for each question using only features corresponding to that question, and the predictions from these models are interpreted as question-level scores.}}{31}{figure.caption.15}\protected@file@percent }
\newlabel{fig:question_level_methodology}{{5.6}{31}{Illustration of the question-level scores methodology for obtaining analytic scores for each question. Separate linear models are trained to predict the same target variable (part score) for each question using only features corresponding to that question, and the predictions from these models are interpreted as question-level scores}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Low Data Regime}{31}{section.5.6}\protected@file@percent }
\newlabel{sec:low_data_regime}{{5.6}{31}{Low Data Regime}{section.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Question-level scores for different rubric dimensions across various sample candidates, illustrating how each criterion contributes to the overall assessment. Each file ID corresponds to a candidate's response for a specific test component.}}{32}{figure.caption.16}\protected@file@percent }
\newlabel{fig:analytic_scores}{{5.7}{32}{Question-level scores for different rubric dimensions across various sample candidates, illustrating how each criterion contributes to the overall assessment. Each file ID corresponds to a candidate's response for a specific test component}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Scaling curves showing PCC as a function of training data size (log scale) across different feature sets. Each point represents the mean performance over 100 random sub-samples of the training data, with the shaded region indicating the standard deviation across the 100 sub-samples.}}{33}{figure.caption.17}\protected@file@percent }
\newlabel{fig:low_data_regime_pcc}{{5.8}{33}{Scaling curves showing PCC as a function of training data size (log scale) across different feature sets. Each point represents the mean performance over 100 random sub-samples of the training data, with the shaded region indicating the standard deviation across the 100 sub-samples}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Scaling curves showing RMSE as a function of training data size (log-scale) across different feature sets. Each point represents the mean performance over 100 random sub-samples of the training data, with the shaded region indicating the standard deviation across the 100 sub-samples.}}{33}{figure.caption.18}\protected@file@percent }
\newlabel{fig: fig:low_data_regime_rmse}{{5.9}{33}{Scaling curves showing RMSE as a function of training data size (log-scale) across different feature sets. Each point represents the mean performance over 100 random sub-samples of the training data, with the shaded region indicating the standard deviation across the 100 sub-samples}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{34}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{6}{34}{Conclusion}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Findings}{34}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Implications for SLA and Educational Technology}{35}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Limitations and Future Work}{35}{section.6.3}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\bibcite{apache_parquet}{{1}{2013}{{apa}}{{}}}
\bibcite{baevski2020wav2vec20frameworkselfsupervised}{{2}{2020}{{Baevski et~al.}}{{Baevski, Zhou, Mohamed, and Auli}}}
\bibcite{balek2025llmbasedfeaturegenerationtext}{{3}{2025}{{Balek et~al.}}{{Balek, Sýkora, Sklenák, and Kliegr}}}
\bibcite{balestriero2023cookbookselfsupervisedlearning}{{4}{2023}{{Balestriero et~al.}}{{Balestriero, Ibrahim, Sobal, Morcos, Shekhar, Goldstein, Bordes, Bardes, Mialon, Tian, Schwarzschild, Wilson, Geiping, Garrido, Fernandez, Bar, Pirsiavash, LeCun, and Goldblum}}}
\bibcite{banno-etal-2024-gpt}{{5}{2024}{{Bann{\`o} et~al.}}{{Bann{\`o}, Vydana, Knill, and Gales}}}
\bibcite{bannò2022proficiencyassessmentl2spoken}{{6}{2022}{{Bannò and Matassoni}}{{}}}
\bibcite{bannò2022l2proficiencyassessmentusing}{{7}{2022}{{Bannò et~al.}}{{Bannò, Knill, Matassoni, Raina, and Gales}}}
\bibcite{beltagy2020longformerlongdocumenttransformer}{{8}{2020}{{Beltagy et~al.}}{{Beltagy, Peters, and Cohan}}}
\bibcite{benara2024craftinginterpretableembeddingsasking}{{9}{2024}{{Benara et~al.}}{{Benara, Singh, Morris, Antonello, Stoica, Huth, and Gao}}}
\bibcite{bernstein90_icslp}{{10}{1990}{{Bernstein et~al.}}{{Bernstein, Cohen, Murveit, Rtischev, and Weintraub}}}
\bibcite{gpt3}{{11}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}}}
\bibcite{cambridge2020linguaskill}{{12}{2020}{{Cambridge English}}{{}}}
\bibcite{chen2020simpleframeworkcontrastivelearning}{{13}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{cheng2023batchpromptingefficientinference}{{14}{2023}{{Cheng et~al.}}{{Cheng, Kasai, and Yu}}}
\bibcite{chu2025rationaleessayscoresenhancing}{{15}{2025}{{Chu et~al.}}{{Chu, Kim, Wong, and Yi}}}
\bibcite{chu2023qwenaudioadvancinguniversalaudio}{{16}{2023}{{Chu et~al.}}{{Chu, Xu, Zhou, Yang, Zhang, Yan, Zhou, and Zhou}}}
\bibcite{chu2024qwen2audiotechnicalreport}{{17}{2024}{{Chu et~al.}}{{Chu, Xu, Yang, Wei, Wei, Guo, Leng, Lv, He, Lin, Zhou, and Zhou}}}
\bibcite{CouncilOfEurope2001CEFR}{{18}{2001}{{Council of Europe}}{{}}}
\bibcite{devlin2019bertpretrainingdeepbidirectional}{{19}{2019}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{do2024autoregressivescoregenerationmultitrait}{{20}{2024}{{Do et~al.}}{{Do, Kim, and Lee}}}
\bibcite{eltanbouly2025tratestraitspecificrubricassistedcrossprompt}{{21}{2025}{{Eltanbouly et~al.}}{{Eltanbouly, Albatarni, and Elsayed}}}
\bibcite{fu2024pronunciationassessmentmultimodallarge}{{22}{2024}{{Fu et~al.}}{{Fu, Peng, Yang, and Zhou}}}
\bibcite{galton_1886_1449548}{{23}{1886}{{Galton}}{{}}}
\bibcite{ghosh2024gamalargeaudiolanguagemodel}{{24}{2024}{{Ghosh et~al.}}{{Ghosh, Kumar, Seth, Evuru, Tyagi, Sakshi, Nieto, Duraiswami, and Manocha}}}
\bibcite{hsu2021hubertselfsupervisedspeechrepresentation}{{25}{2021}{{Hsu et~al.}}{{Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and Mohamed}}}
\bibcite{hu2021loralowrankadaptationlarge}{{26}{2021}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{kim2022automaticpronunciationassessmentusing}{{27}{2022}{{Kim et~al.}}{{Kim, Jeon, Seo, and Kim}}}
\bibcite{knill2024sandi}{{28}{2024}{{Knill et~al.}}{{Knill, Nicholls, Gales, Qian, and Stroinski}}}
\bibcite{kwon2023efficientmemorymanagementlarge}{{29}{2023}{{Kwon et~al.}}{{Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica}}}
\bibcite{lee2024unleashinglargelanguagemodels}{{30}{2024}{{Lee et~al.}}{{Lee, Cai, Meng, Wang, and Wu}}}
\bibcite{liusie2023mitigatingwordbiaszeroshot}{{31}{2023}{{Liusie et~al.}}{{Liusie, Manakul, and Gales}}}
\bibcite{Ludlow2020OfficialQuick}{{32}{2020}{{Ludlow}}{{}}}
\bibcite{ma2025assessment}{{33}{2025}{{Ma et~al.}}{{Ma, Qian, Tang, Bannò, Knill, and Gales}}}
\bibcite{mansour2024largelanguagemodelsautomatically}{{34}{2024}{{Mansour et~al.}}{{Mansour, Albatarni, Eltanbouly, and Elsayed}}}
\bibcite{mathias-bhattacharyya-2018-asap}{{35}{2018}{{Mathias and Bhattacharyya}}{{}}}
\bibcite{mcinerney2023chillzeroshotcustominterpretable}{{36}{2023}{{McInerney et~al.}}{{McInerney, Young, van~de Meent, and Wallace}}}
\bibcite{mcknight_civelekoglu_gales_banno_liusie_knill_2023}{{37}{2023}{{McKnight et~al.}}{{McKnight, Civelekoglu, Gales, Banno, Liusie, and Knill}}}
\bibcite{microsoft2025phi4minitechnicalreportcompact}{{38}{2025}{{Microsoft et~al.}}{{Microsoft, :, Abouelenin, Ashfaq, Atkinson, Awadalla, Bach, Bao, Benhaim, Cai, Chaudhary, Chen, Chen, Chen, Chen, Chen, Chen, ling Chen, Dai, Dai, Fan, Gao, Gao, Garg, Goswami, Hao, Hendy, Hu, Jin, Khademi, Kim, Kim, Lee, Li, Li, Liang, Lin, Lin, Liu, Liu, Lopez, Luo, Madan, Mazalov, Mitra, Mousavi, Nguyen, Pan, Perez-Becker, Platin, Portet, Qiu, Ren, Ren, Roy, Shang, Shen, Singhal, Som, Song, Sych, Vaddamanu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yu, Zabir, Zhang, Zhang, Zhang, and Zhou}}}
\bibcite{aes_gpt3}{{39}{2023}{{Mizumoto and Eguchi}}{{}}}
\bibcite{naismith-etal-2023-automated}{{40}{2023}{{Naismith et~al.}}{{Naismith, Mulcaire, and Burstein}}}
\bibcite{openai2024gpt4technicalreport}{{41}{2024}{{OpenAI et~al.}}{{OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk, Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone, Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph}}}
\bibcite{page1966}{{42}{1966}{{Page}}{{}}}
\bibcite{scikit-learn}{{43}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{peng21e_interspeech}{{44}{2021}{{Peng et~al.}}{{Peng, Fu, Lin, Ke, and Zhan}}}
\bibcite{qian2024sandi}{{45}{2024}{{Qian et~al.}}{{Qian, Knill, Bannò, Tang, Karanasou, Gales, and Nicholls}}}
\bibcite{qian12c_interspeech}{{46}{2012}{{Qian et~al.}}{{Qian, Meng, and Soong}}}
\bibcite{radford2022robustspeechrecognitionlargescale}{{47}{2022}{{Radford et~al.}}{{Radford, Kim, Xu, Brockman, McLeavey, and Sutskever}}}
\bibcite{raffel2023exploringlimitstransferlearning}{{48}{2023}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}}}
\bibcite{raina20_interspeech}{{49}{2020}{{Raina et~al.}}{{Raina, Gales, and Knill}}}
\bibcite{rubenstein2023audiopalmlargelanguagemodel}{{50}{2023}{{Rubenstein et~al.}}{{Rubenstein, Asawaroengchai, Nguyen, Bapna, Borsos, de~Chaumont~Quitry, Chen, Badawy, Han, Kharitonov, Muckenhirn, Padfield, Qin, Rozenberg, Sainath, Schalkwyk, Sharifi, Ramanovich, Tagliasacchi, Tudor, Velimirović, Vincent, Yu, Wang, Zayats, Zeghidour, Zhang, Zhang, Zilka, and Frank}}}
\bibcite{sakshi2024mmaumassivemultitaskaudio}{{51}{2024}{{Sakshi et~al.}}{{Sakshi, Tyagi, Kumar, Seth, Selvakumar, Nieto, Duraiswami, Ghosh, and Manocha}}}
\bibcite{sam2025predictingperformanceblackboxllms}{{52}{2025}{{Sam et~al.}}{{Sam, Finzi, and Kolter}}}
\bibcite{stahl2024exploringllmpromptingstrategies}{{53}{2024}{{Stahl et~al.}}{{Stahl, Biermann, Nehring, and Wachsmuth}}}
\bibcite{tang2024salmonngenerichearingabilities}{{54}{2024}{{Tang et~al.}}{{Tang, Yu, Sun, Chen, Tan, Li, Lu, Ma, and Zhang}}}
\bibcite{touvron2023llama2openfoundation}{{55}{2023}{{Touvron et~al.}}{{Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom}}}
\bibcite{bert_1}{{56}{2021}{{Wang et~al.}}{{Wang, Evanini, Qian, and Mulholland}}}
\bibcite{wolf2020huggingfacestransformersstateoftheartnatural}{{57}{2020}{{Wolf et~al.}}{{Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}}}
\bibcite{inproceedings_wu_mdd}{{58}{2021}{{Wu et~al.}}{{Wu, Li, Leung, and Meng}}}
\bibcite{xu2025qwen25omnitechnicalreport}{{59}{2025}{{Xu et~al.}}{{Xu, Guo, He, Hu, He, Bai, Chen, Wang, Fan, Dang, Zhang, Wang, Chu, and Lin}}}
\bibcite{xu21k_interspeech}{{60}{2021}{{Xu et~al.}}{{Xu, Kang, Cao, Lin, and Ma}}}
\bibcite{yancey-etal-2023-rating}{{61}{2023}{{Yancey et~al.}}{{Yancey, Laflair, Verardi, and Burstein}}}
\bibcite{yang2020xlnetgeneralizedautoregressivepretraining}{{62}{2020}{{Yang et~al.}}{{Yang, Dai, Yang, Carbonell, Salakhutdinov, and Le}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Full Question Sets and Prompts}{45}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:full_question_sets}{{A}{45}{Full Question Sets and Prompts}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Initial Question Set}{45}{section.A.1}\protected@file@percent }
\newlabel{app_sec:initial_question_set}{{A.1}{45}{Initial Question Set}{section.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Direct Scoring Prompt}{47}{section.A.2}\protected@file@percent }
\newlabel{app_sec:direct_scoring_prompt}{{A.2}{47}{Direct Scoring Prompt}{section.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Rubric-based Question Set}{49}{section.A.3}\protected@file@percent }
\newlabel{app_sec:rubric_based_question_set}{{A.3}{49}{Rubric-based Question Set}{section.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Rubric-based Question Set Batch Prompt}{50}{section.A.4}\protected@file@percent }
\newlabel{app_sec:rubric_based_question_set_batch}{{A.4}{50}{Rubric-based Question Set Batch Prompt}{section.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Revised Question Set}{52}{section.A.5}\protected@file@percent }
\newlabel{app_sec:revised_question_set}{{A.5}{52}{Revised Question Set}{section.A.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Revised Question Set Batch Prompt}{56}{section.A.6}\protected@file@percent }
\newlabel{app_sec:revised_question_set_batch}{{A.6}{56}{Revised Question Set Batch Prompt}{section.A.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Results}{61}{appendix.B}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:additional_results}{{B}{61}{Additional Results}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Development Subset}{61}{section.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Part 1}{61}{subsection.B.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 1 only).}}{61}{table.caption.30}\protected@file@percent }
\newlabel{table:part1_results}{{B.1}{61}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 1 only)}{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}Part 3}{62}{subsection.B.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 3 only).}}{62}{table.caption.31}\protected@file@percent }
\newlabel{table:part3_results}{{B.2}{62}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 3 only)}{table.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Part 4}{62}{subsection.B.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 4 only).}}{62}{table.caption.32}\protected@file@percent }
\newlabel{table:part4_results}{{B.3}{62}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 4 only)}{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.4}Part 5}{62}{subsection.B.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 5 only).}}{62}{table.caption.33}\protected@file@percent }
\newlabel{table:part5_results}{{B.4}{62}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for part 5 only)}{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.5}Overall}{62}{subsection.B.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.5}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on dev set for overall scores).}}{63}{table.caption.34}\protected@file@percent }
\newlabel{table:overall_results}{{B.5}{63}{Performance comparison of linear regression models across different feature representations (evaluated on dev set for overall scores)}{table.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Evaluation Subset}{63}{section.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Part 1}{63}{subsection.B.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.6}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 1 only).}}{63}{table.caption.35}\protected@file@percent }
\newlabel{table:part1_eval_results}{{B.6}{63}{Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 1 only)}{table.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Part 3}{63}{subsection.B.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.7}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 3 only).}}{63}{table.caption.36}\protected@file@percent }
\newlabel{table:part3_eval_results}{{B.7}{63}{Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 3 only)}{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.3}Part 4}{64}{subsection.B.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.8}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 4 only).}}{64}{table.caption.37}\protected@file@percent }
\newlabel{table:part4_eval_results}{{B.8}{64}{Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 4 only)}{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.4}Part 5}{64}{subsection.B.2.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.9}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 5 only).}}{64}{table.caption.38}\protected@file@percent }
\newlabel{table:part5_eval_results}{{B.9}{64}{Performance comparison of linear regression models across different feature representations (evaluated on eval set for part 5 only)}{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.5}Overall}{64}{subsection.B.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.10}{\ignorespaces Performance comparison of linear regression models across different feature representations (evaluated on eval set for overall scores).}}{64}{table.caption.39}\protected@file@percent }
\newlabel{table:overall_eval_results}{{B.10}{64}{Performance comparison of linear regression models across different feature representations (evaluated on eval set for overall scores)}{table.caption.39}{}}
\gdef \@abspage@last{67}
